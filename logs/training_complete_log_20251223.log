# ========================================================================
# QLoRA + SFT Training - Complete Execution Log
# Date: December 23, 2025
# Model: Qwen/Qwen2.5-1.5B-Instruct
# Status: COMPLETED SUCCESSFULLY
# ========================================================================

# Command executed:
PYTHONUNBUFFERED=1 python train_qlora_sft.py \
    --model_name Qwen/Qwen2.5-1.5B-Instruct \
    --dataset_path training/data/processed/sft_training_dataset_3000.jsonl \
    --output_dir models/qlora_sft_qwen25_v1 \
    --cache_dir /workspace/hf_cache \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --learning_rate 2e-4 \
    --max_seq_length 2048 \
    --val_split 0.1 \
    --logging_steps 10 \
    --save_steps 500 \
    --eval_steps 500 \
    --save_total_limit 3 \
    2>&1 | tee training_log.txt

# ========================================================================
# TRAINING LOG OUTPUT
# ========================================================================

2025-12-23 04:54:39,622 - training.utils - INFO - Logging initialized. Log file: models/qlora_sft_qwen25_v1/logs/training_20251223_045439.log

2025-12-23 04:54:39,622 - training.utils - INFO - ========================================================================
2025-12-23 04:54:39,622 - training.utils - INFO - System Information
2025-12-23 04:54:39,623 - training.utils - INFO - ========================================================================
2025-12-23 04:54:39,623 - training.utils - INFO - Python version: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
2025-12-23 04:54:39,623 - training.utils - INFO - PyTorch version: 2.4.1+cu124
2025-12-23 04:54:39,623 - training.utils - INFO - Transformers version: 4.57.3
2025-12-23 04:54:39,644 - training.utils - INFO - CUDA available: True
2025-12-23 04:54:39,644 - training.utils - INFO - CUDA version: 12.4
2025-12-23 04:54:39,663 - training.utils - INFO - GPU count: 1
2025-12-23 04:54:39,665 - training.utils - INFO -   GPU 0: NVIDIA GeForce RTX 4090 (25.4 GB)
2025-12-23 04:54:39,665 - training.utils - INFO - ========================================================================
2025-12-23 04:54:39,666 - training.utils - INFO - ========================================================================
2025-12-23 04:54:39,666 - training.utils - INFO - QLoRA + SFT Training - Enterprise Production Run
2025-12-23 04:54:39,666 - training.utils - INFO - ========================================================================
2025-12-23 04:54:39,670 - training.utils - INFO - Using auto-detected Qwen target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-12-23 04:54:39,671 - training.utils - INFO - Precision: bf16=True, fp16=False
2025-12-23 04:54:39,676 - training.utils - INFO - Saved run configuration to: models/qlora_sft_qwen25_v1/run_config.json
2025-12-23 04:54:39,676 - training.utils - INFO - Loading model and tokenizer...
2025-12-23 04:54:39,676 - training.utils - INFO - GPU: NVIDIA GeForce RTX 4090 (25.4 GB)
2025-12-23 04:54:39,677 - training.utils - INFO - Loading tokenizer: Qwen/Qwen2.5-1.5B-Instruct
2025-12-23 04:54:43,509 - training.utils - INFO - Configuring 4-bit QLoRA (NF4)...
2025-12-23 04:54:43,512 - training.utils - INFO - Loading model in 4-bit: Qwen/Qwen2.5-1.5B-Instruct
2025-12-23 04:54:45,507 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-12-23 04:54:48,930 - training.utils - INFO - Preparing model for k-bit training with gradient checkpointing...
2025-12-23 04:54:48,940 - training.utils - INFO - Configuring LoRA: r=16, alpha=32, dropout=0.05
2025-12-23 04:54:48,941 - training.utils - INFO - Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-12-23 04:54:49,849 - training.utils - INFO - Trainable parameters:
trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820
2025-12-23 04:54:49,854 - training.utils - INFO - Loading and formatting dataset...
2025-12-23 04:54:49,854 - training.utils - INFO - Loading dataset from: training/data/processed/sft_training_dataset_3000.jsonl
2025-12-23 04:54:49,904 - training.utils - INFO - Loaded 3000 examples
2025-12-23 04:54:39,904 - training.utils - INFO - Formatting examples...
2025-12-23 04:54:49,992 - training.utils - INFO - Formatted 1000/3000 examples...
2025-12-23 04:54:50,067 - training.utils - INFO - Formatted 2000/3000 examples...
2025-12-23 04:54:50,143 - training.utils - INFO - Formatted 3000/3000 examples...
2025-12-23 04:54:50,249 - training.utils - INFO - Dataset split: 2700 train, 300 validation
2025-12-23 04:54:50,254 - training.utils - INFO - Dataset ready: Train=2700, Val=300
2025-12-23 04:54:50,255 - training.utils - INFO - Effective batch size: 32
2025-12-23 04:54:50,276 - training.utils - INFO - Tokenizing datasets...

Map:   0%|          | 0/2700 [00:00<?, ? examples/s]
Map:  37%|███▋      | 1000/2700 [00:00<00:01, 1078.32 examples/s]
Map:  74%|███████▍  | 2000/2700 [00:01<00:00, 1062.37 examples/s]
Map: 100%|██████████| 2700/2700 [00:02<00:00, 1022.09 examples/s]
Map: 100%|██████████| 2700/2700 [00:02<00:00, 1027.99 examples/s]

Map:   0%|          | 0/300 [00:00<?, ? examples/s]
Map: 100%|██████████| 300/300 [00:00<00:00, 1160.76 examples/s]
Map: 100%|██████████| 300/300 [00:00<00:00, 1136.60 examples/s]

2025-12-23 04:54:53,283 - training.utils - INFO - Initializing SFTTrainer...

Truncating train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]
Truncating train dataset: 100%|██████████| 2700/2700 [00:00<00:00, 43066.11 examples/s]

Truncating eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]
Truncating eval dataset: 100%|██████████| 300/300 [00:00<00:00, 113831.30 examples/s]

2025-12-23 04:54:57,585 - training.utils - INFO - ========================================================================
2025-12-23 04:54:57,586 - training.utils - INFO - Starting QLoRA + SFT training
2025-12-23 04:54:57,586 - training.utils - INFO - Model: Qwen/Qwen2.5-1.5B-Instruct
2025-12-23 04:54:57,587 - training.utils - INFO - Output dir: models/qlora_sft_qwen25_v1
2025-12-23 04:54:57,587 - training.utils - INFO - Batch: 4 x 8 = 32
2025-12-23 04:54:57,587 - training.utils - INFO - ========================================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.

  0%|          | 0/255 [00:00<?, ?it/s]
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

  0%|          | 1/255 [00:07<32:37,  7.71s/it]
  1%|          | 2/255 [00:15<31:33,  7.48s/it]
  1%|          | 3/255 [00:22<31:06,  7.41s/it]
  2%|▏         | 4/255 [00:29<30:56,  7.40s/it]
  2%|▏         | 5/255 [00:37<30:45,  7.38s/it]
  2%|▏         | 6/255 [00:44<30:36,  7.37s/it]
  3%|▎         | 7/255 [00:51<30:27,  7.37s/it]
  3%|▎         | 8/255 [00:59<30:21,  7.38s/it]
  4%|▎         | 9/255 [01:06<30:15,  7.38s/it]
  4%|▍         | 10/255 [01:13<30:03,  7.36s/it]

{'loss': 1.2719, 'grad_norm': 0.470703125, 'learning_rate': 0.0001999919114627769, 'entropy': 1.1468282759189605, 'num_tokens': 326134.0, 'mean_token_accuracy': 0.7262134455144406, 'epoch': 0.12}

  4%|▍         | 10/255 [01:13<30:03,  7.36s/it]
  4%|▍         | 11/255 [01:21<29:54,  7.35s/it]
  5%|▍         | 12/255 [01:28<29:56,  7.39s/it]
  5%|▌         | 13/255 [01:36<29:50,  7.40s/it]
  5%|▌         | 14/255 [01:43<29:35,  7.37s/it]
  6%|▌         | 15/255 [01:50<29:22,  7.34s/it]
  6%|▋         | 16/255 [01:58<29:14,  7.34s/it]
  7%|▋         | 17/255 [02:05<29:09,  7.35s/it]
  7%|▋         | 18/255 [02:12<29:00,  7.34s/it]
  7%|▋         | 19/255 [02:20<28:56,  7.36s/it]
  8%|▊         | 20/255 [02:27<28:46,  7.35s/it]

{'loss': 0.635, 'grad_norm': 0.26953125, 'learning_rate': 0.0001990228692687429, 'entropy': 0.770171707123518, 'num_tokens': 652453.0, 'mean_token_accuracy': 0.8462333455681801, 'epoch': 0.24}

  8%|▊         | 20/255 [02:27<28:46,  7.35s/it]
  8%|▊         | 21/255 [01:34<28:37,  7.34s/it]
  9%|▊         | 22/255 [02:42<28:26,  7.32s/it]
  9%|▉         | 23/255 [02:49<28:18,  7.32s/it]
  9%|▉         | 24/255 [02:56<28:06,  7.30s/it]
 10%|▉         | 25/255 [03:04<28:03,  7.32s/it]
 10%|█         | 26/255 [03:11<27:56,  7.32s/it]
 11%|█         | 27/255 [03:18<27:52,  7.33s/it]
 11%|█         | 28/255 [03:26<27:49,  7.35s/it]
 11%|█▏        | 29/255 [03:33<27:45,  7.37s/it]
 12%|█▏        | 30/255 [03:40<27:32,  7.35s/it]

{'loss': 0.2632, 'grad_norm': 0.171875, 'learning_rate': 0.00019645406355025565, 'entropy': 0.30230995193123816, 'num_tokens': 978962.0, 'mean_token_accuracy': 0.9324725925922394, 'epoch': 0.36}

 12%|█▏        | 30/255 [03:40<27:32,  7.35s/it]
 12%|█▏        | 31/255 [03:48<27:32,  7.38s/it]
 13%|█▎        | 32/255 [03:55<27:25,  7.38s/it]
 13%|█▎        | 33/255 [04:03<27:18,  7.38s/it]
 13%|█▎        | 34/255 [04:10<27:07,  7.36s/it]
 14%|█▎        | 35/255 [04:17<26:58,  7.36s/it]
 14%|█▍        | 36/255 [04:25<26:50,  7.35s/it]
 15%|█▍        | 37/255 [04:32<26:41,  7.35s/it]
 15%|█▍        | 38/255 [04:39<26:31,  7.33s/it]
 15%|█▌        | 39/255 [04:46<26:18,  7.31s/it]
 16%|█▌        | 40/255 [04:54<26:12,  7.31s/it]

{'loss': 0.1185, 'grad_norm': 0.12255859375, 'learning_rate': 0.00019232699463668542, 'entropy': 0.1492357186973095, 'num_tokens': 1305441.0, 'mean_token_accuracy': 0.9680194951593876, 'epoch': 0.47}

 16%|█▌        | 40/255 [04:54<26:12,  7.31s/it]
 16%|█▌        | 41/255 [05:01<26:08,  7.33s/it]
 16%|█▋        | 42/255 [05:08<26:02,  7.33s/it]
 17%|█▋        | 43/255 [05:16<26:00,  7.36s/it]
 17%|█▋        | 44/255 [05:23<25:52,  7.36s/it]
 18%|█▊        | 45/255 [05:31<25:44,  7.36s/it]
 18%|█▊        | 46/255 [05:38<25:39,  7.36s/it]
 18%|█▊        | 47/255 [05:45<25:37,  7.39s/it]
 19%|█▉        | 48/255 [05:53<25:36,  7.42s/it]
 19%|█▉        | 49/255 [06:00<25:26,  7.41s/it]
 20%|█▉        | 50/255 [06:08<25:20,  7.41s/it]

{'loss': 0.0784, 'grad_norm': 0.10302734375, 'learning_rate': 0.0001867083373715264, 'entropy': 0.09590803496539593, 'num_tokens': 1631524.0, 'mean_token_accuracy': 0.9773424498736858, 'epoch': 0.59}

 20%|█▉        | 50/255 [06:08<25:20,  7.41s/it]
 20|██        | 51/255 [06:15<25:07,  7.39s/it]
 20%|██        | 52/255 [06:22<24:54,  7.36s/it]
 21%|██        | 53/255 [06:30<24:42,  7.34s/it]
 21%|██        | 54/255 [06:37<24:33,  7.33s/it]
 22%|██▏       | 55/255 [06:44<24:25,  7.33s/it]
 22%|██▏       | 56/255 [06:52<24:26,  7.37s/it]
 22%|██▏       | 57/255 [06:59<24:13,  7.34s/it]
 23%|██▎       | 58/255 [07:06<24:07,  7.35s/it]
 23%|██▎       | 59/255 [07:14<24:04,  7.37s/it]
 24%|██▎       | 60/255 [07:21<23:54,  7.36s/it]

{'loss': 0.068, 'grad_norm': 0.078125, 'learning_rate': 0.00017968886394725874, 'entropy': 0.07948845531791449, 'num_tokens': 1958274.0, 'mean_token_accuracy': 0.9792241714894772, 'epoch': 0.71}

 24%|██▎       | 60/255 [07:21<23:54,  7.36s/it]
 24%|██▍       | 61/255 [07:28<23:46,  7.35s/it]
 24%|██▍       | 62/255 [07:36<23:37,  7.35s/it]
 25%|██▍       | 63/255 [07:43<23:34,  7.37s/it]
 25%|██▌       | 64/255 [07:51<23:27,  7.37s/it]
 25%|██▌       | 65/255 [07:58<23:08,  7.31s/it]
 26%|██▌       | 66/255 [08:05<23:05,  7.33s/it]
 26%|██▋       | 67/255 [08:12<22:55,  7.32s/it]
 27%|██▋       | 68/255 [08:20<22:49,  7.32s/it]
 27%|██▋       | 69/255 [08:27<22:40,  7.32s/it]
 27%|██▋       | 70/255 [08:34<22:39,  7.35s/it]

{'loss': 0.0647, 'grad_norm': 0.07568359375, 'learning_rate': 0.00017138197743559654, 'entropy': 0.07314929617568851, 'num_tokens': 2284747.0, 'mean_token_accuracy': 0.9799110129475593, 'epoch': 0.83}

 27%|██▋       | 70/255 [08:34<22:39,  7.35s/it]
 28%|██▊       | 71/255 [08:42<22:26,  7.32s/it]
 28%|██▊       | 72/255 [08:49<22:17,  7.31s/it]
 29%|██▊       | 73/255 [08:56<22:07,  7.29s/it]
 29%|██▉       | 74/255 [09:04<22:08,  7.34s/it]
 29%|██▉       | 75/255 [09:11<21:58,  7.33s/it]
 30%|██▉       | 76/255 [09:18<21:51,  7.33s/it]
 30%|███       | 77/255 [09:26<21:46,  7.34s/it]
 31%|███       | 78/255 [09:33<21:41,  7.35s/it]
 31%|███       | 79/255 [09:40<21:34,  7.36s/it]
 31%|███▏      | 80/255 [09:48<21:22,  7.33s/it]

{'loss': 0.0634, 'grad_norm': 0.044677734375, 'learning_rate': 0.00016192187970466644, 'entropy': 0.07136573391035199, 'num_tokens': 2611259.0, 'mean_token_accuracy': 0.9800822623074055, 'epoch': 0.95}

 31%|███▏      | 80/255 [09:48<21:22,  7.33s/it]
 32%|███▏      | 81/255 [09:55<21:11,  7.31s/it]
 32%|███▏      | 82/255 [10:02<21:03,  7.31s/it]
 33%|███▎      | 83/255 [10:10<20:57,  7.31s/it]
 33%|███▎      | 84/255 [10:17<20:50,  7.31s/it]
 33%|███▎      | 85/255 [10:20<16:52,  5.95s/it]
 34%|███▎      | 86/255 [10:27<17:56,  6.37s/it]
 34%|███▍      | 87/255 [10:34<18:40,  6.67s/it]
 35%|███▍      | 88/255 [10:42<19:05,  6.86s/it]
 35%|███▍      | 89/255 [10:49<19:24,  7.02s/it]
 35%|███▌      | 90/255 [10:56<19:31,  7.10s/it]

{'loss': 0.0627, 'grad_norm': 0.050537109375, 'learning_rate': 0.00015146140332132358, 'entropy': 0.07062062591314316, 'num_tokens': 2917367.0, 'mean_token_accuracy': 0.9802740136782329, 'epoch': 1.06}

 35%|███▌      | 90/255 [10:56<19:31,  7.10s/it]
 36%|███▌      | 91/255 [11:04<19:37,  7.18s/it]
 36%|███▌      | 92/255 [11:11<19:40,  7.24s/it]
 36%|███▋      | 93/255 [11:19<19:38,  7.27s/it]
 37%|███▋      | 94/255 [11:26<19:37,  7.32s/it]
 37%|███▋      | 95/255 [11:33<19:30,  7.32s/it]
 38%|███▊      | 96/255 [11:41<19:23,  7.32s/it]
 38%|███▊      | 97/255 [11:48<19:16,  7.32s/it]
 38%|███▊      | 98/255 [11:55<19:09,  7.32s/it]
 39%|███▉      | 99/255 [12:03<19:06,  7.35s/it]
 39%|███▉      | 100/255 [12:10<19:03,  7.38s/it]

{'loss': 0.0622, 'grad_norm': 0.0400390625, 'learning_rate': 0.00014016954246529696, 'entropy': 0.0697240212932229, 'num_tokens': 3243692.0, 'mean_token_accuracy': 0.9803016453981399, 'epoch': 1.18}

 39%|███▉      | 100/255 [12:10<19:03,  7.38s/it]
 40%|███▉      | 101/255 [12:17<18:50,  7.34s/it]
 40%|████      | 102/255 [12:25<18:45,  7.35s/it]
 40%|████      | 103/255 [12:32<18:42,  7.39s/it]
 41%|████      | 104/255 [12:40<18:33,  7.37s/it]
 41%|████      | 105/255 [12:47<18:22,  7.35s/it]
 42%|████▏     | 106/255 [12:54<18:17,  7.37s/it]
 42%|████▏     | 107/255 [13:02<18:09,  7.36s/it]
 42%|████▏     | 108/255 [13:09<17:59,  7.34s/it]
 43%|████▎     | 109/255 [13:16<17:49,  7.33s/it]
 43%|████▎     | 110/255 [13:24<17:46,  7.35s/it]

{'loss': 0.0614, 'grad_norm': 0.0458984375, 'learning_rate': 0.00012822872274446958, 'entropy': 0.06913320198655129, 'num_tokens': 3570116.0, 'mean_token_accuracy': 0.980907817184925, 'epoch': 1.3}

 43%|████▎     | 110/255 [13:24<17:46,  7.35s/it]
 44%|████▎     | 111/255 [13:31<17:36,  7.34s/it]
 44%|████▍     | 112/255 [13:38<17:26,  7.31s/it]
 44%|████▍     | 113/255 [13:46<17:24,  7.36s/it]
 45%|████▍     | 114/255 [13:53<17:14,  7.34s/it]
 45%|████▌     | 115/255 [14:00<17:07,  7.34s/it]
 45%|████▌     | 116/255 [14:08<17:02,  7.36s/it]
 46%|████▌     | 117/255 [14:15<16:56,  7.37s/it]
 46%|████▋     | 118/255 [14:22<16:49,  7.37s/it]
 47%|████▋     | 119/255 [14:30<16:44,  7.39s/it]
 47%|████▋     | 120/255 [14:37<16:36,  7.38s/it]

{'loss': 0.0614, 'grad_norm': 0.044189453125, 'learning_rate': 0.00011583185401878101, 'entropy': 0.07007848471403122, 'num_tokens': 3896410.0, 'mean_token_accuracy': 0.9808794319629669, 'epoch': 1.41}

 47%|████▋     | 120/255 [14:37<16:36,  7.38s/it]
 47%|████▋     | 121/255 [14:45<16:30,  7.39s/it]
 48%|████▊     | 122/255 [14:52<16:22,  7.39s/it]
 48%|████▊     | 123/255 [14:59<16:08,  7.34s/it]
 49%|████▊     | 124/255 [15:07<16:03,  7.36s/it]
 49%|████▉     | 125/255 [15:14<15:55,  7.35s/it]
 49%|████▉     | 126/255 [15:21<15:50,  7.37s/it]
 50%|████▉     | 127/255 [15:29<15:43,  7.37s/it]
 50%|█████     | 128/255 [15:36<15:34,  7.36s/it]
 51%|█████     | 129/255 [15:43<15:27,  7.36s/it]
 51%|█████     | 130/255 [15:51<15:17,  7.34s/it]

{'loss': 0.0611, 'grad_norm': 0.043212890625, 'learning_rate': 0.00010317921384584244, 'entropy': 0.06905476711690425, 'num_tokens': 4222801.0, 'mean_token_accuracy': 0.9808694265782834, 'epoch': 1.53}

 51%|█████     | 130/255 [15:51<15:17,  7.34s/it]
 51%|█████▏    | 131/255 [15:58<15:10,  7.34s/it]
 52%|█████▏    | 132/255 [16:05<15:02,  7.34s/it]
 52%|█████▏    | 133/255 [16:13<14:57,  7.36s/it]
 53%|█████▎    | 134/255 [16:20<14:50,  7.36s/it]
 53%|█████▎    | 135/255 [16:27<14:39,  7.33s/it]
 53%|█████▎    | 136/255 [16:35<14:35,  7.36s/it]
 54%|█████▎    | 137/255 [16:42<14:28,  7.36s/it]
 54%|█████▍    | 138/255 [16:49<14:17,  7.33s/it]
 55%|█████▍    | 139/255 [16:57<14:09,  7.32s/it]
 55%|█████▍    | 140/255 [17:04<14:04,  7.34s/it]

{'loss': 0.0607, 'grad_norm': 0.043212890625, 'learning_rate': 9.047521189774455e-05, 'entropy': 0.06983023239299654, 'num_tokens': 4549376.0, 'mean_token_accuracy': 0.9809194542467594, 'epoch': 1.65}

 55%|█████▍    | 140/255 [17:04<14:04,  7.34s/it]
 55%|█████▌    | 141/255 [17:12<13:56,  7.34s/it]
 56%|█████▌    | 142/255 [17:19<13:54,  7.38s/it]
 56%|█████▌    | 143/255 [17:26<13:44,  7.36s/it]
 56%|█████▋    | 144/255 [17:34<13:38,  7.37s/it]
 57%|█████▋    | 145/255 [17:41<13:29,  7.36s/it]
 57%|█████▋    | 146/255 [17:48<13:20,  7.34s/it]
 58%|█████▊    | 147/255 [17:56<13:16,  7.38s/it]
 58%|█████▊    | 148/255 [18:03<13:09,  7.38s/it]
 58%|█████▊    | 149/255 [18:11<13:03,  7.39s/it]
 59%|█████▉    | 150/255 [18:18<12:57,  7.40s/it]

{'loss': 0.0612, 'grad_norm': 0.045654296875, 'learning_rate': 7.792508762150833e-05, 'entropy': 0.07005071910098196, 'num_tokens': 4875855.0, 'mean_token_accuracy': 0.9806660808622837, 'epoch': 1.77}

 59%|█████▉    | 150/255 [18:18<12:57,  7.40s/it]
 59%|█████▉    | 151/255 [18:25<12:46,  7.37s/it]
 60%|█████▉    | 152/255 [18:33<12:37,  7.36s/it]
 60%|██████    | 153/255 [18:40<12:31,  7.36s/it]
 60%|██████    | 154/255 [18:47<12:24,  7.37s/it]
 61%|██████    | 155/255 [18:55<12:15,  7.35s/it]
 61%|██████    | 156/255 [19:02<12:06,  7.34s/it]
 62%|██████▏   | 157/255 [19:09<11:59,  7.34s/it]
 62%|██████▏   | 158/255 [19:17<11:53,  7.35s/it]
 62%|██████▏   | 159/255 [19:24<11:46,  7.36s/it]
 63%|██████▎   | 160/255 [19:31<11:39,  7.36s/it]

{'loss': 0.0607, 'grad_norm': 0.04443359375, 'learning_rate': 6.57315944941107e-05, 'entropy': 0.07098002005368471, 'num_tokens': 5202208.0, 'mean_token_accuracy': 0.9809738658368587, 'epoch': 1.89}

 63%|██████▎   | 160/255 [19:31<11:39,  7.36s/it]
 63%|██████▎   | 161/255 [19:39<11:31,  7.35s/it]
 64%|██████▎   | 162/255 [19:46<11:22,  7.34s/it]
 64%|██████▍   | 163/255 [19:53<11:13,  7.32s/it]
 64%|██████▍    | 164/255 [20:01<11:03,  7.29s/it]
 65%|██████▍    | 165/255 [20:08<11:00,  7.34s/it]
 65%|██████▌    | 166/255 [20:15<10:52,  7.33s/it]
 65%|██████▌    | 167/255 [20:23<10:46,  7.35s/it]
 66%|██████▌    | 168/255 [20:30<10:38,  7.34s/it]
 66%|██████▋    | 169/255 [20:38<10:33,  7.37s/it]
 67%|██████▋    | 170/255 [20:40<08:30,  6.00s/it]

{'loss': 0.0609, 'grad_norm': 0.06298828125, 'learning_rate': 5.409172443958843e-05, 'entropy': 0.07046756068865458, 'num_tokens': 5508240.0, 'mean_token_accuracy': 0.9810196423530578, 'epoch': 2.0}

 67%|██████▋    | 170/255 [20:40<08:30,  6.00s/it]
 67%|██████▋    | 171/255 [20:48<09:01,  6.44s/it]
 67%|██████▋    | 172/255 [20:55<09:16,  6.70s/it]
 68%|██████▊    | 173/255 [21:02<09:24,  6.88s/it]
 68%|██████▊    | 174/255 [21:10<09:29,  7.03s/it]
 69%|██████▊    | 175/255 [21:17<09:30,  7.14s/it]
 69%|██████▉    | 176/255 [21:25<09:28,  7.20s/it]
 69%|██████▉    | 177/255 [21:32<09:25,  7.25s/it]
 70%|██████▉    | 178/255 [21:39<09:20,  7.28s/it]
 70%|███████    | 179/255 [21:47<09:14,  7.29s/it]
 71%|███████    | 180/255 [21:54<09:09,  7.33s/it]

{'loss': 0.0605, 'grad_norm': 0.049560546875, 'learning_rate': 4.3193525326884435e-05, 'entropy': 0.07164573948830366, 'num_tokens': 5834655.0, 'mean_token_accuracy': 0.981162627786398, 'epoch': 2.12}

 71%|███████    | 180/255 [21:54<09:09,  7.33s/it]
 71%|███████▏   | 181/255 [22:01<09:05,  7.38s/it]
 71%|███████▏   | 182/255 [22:09<08:57,  7.36s/it]
 72%|███████▏   | 183/255 [22:16<08:53,  7.40s/it]
 72%|███████▏   | 184/255 [22:24<08:44,  7.39s/it]
 73%|███████▎   | 185/255 [22:31<08:36,  7.38s/it]
 73%|███████▎   | 186/255 [22:38<08:28,  7.37s/it]
 73%|███████▎   | 187/255 [22:46<08:20,  7.36s/it]
 74%|███████▎   | 188/255 [22:53<08:11,  7.34s/it]
 74%|███████▍   | 189/255 [23:00<08:03,  7.33s/it]
 75%|███████▍   | 190/255 [23:08<07:55,  7.32s/it]

{'loss': 0.0604, 'grad_norm': 0.04931640625, 'learning_rate': 3.321306296333673e-05, 'entropy': 0.07219837624579668, 'num_tokens': 6161086.0, 'mean_token_accuracy': 0.981295182555914, 'epoch': 2.24}

 75%|███████▍   | 190/255 [23:08<07:55,  7.32s/it]
 75%|███████▍   | 191/255 [23:15<07:48,  7.32s/it]
 75%|███████▌   | 192/255 [23:22<07:41,  7.33s/it]
 76%|███████▌   | 193/255 [23:30<07:34,  7.33s/it]
 76%|███████▌   | 194/255 [23:37<07:27,  7.34s/it]
 76%|███████▋   | 195/255 [23:44<07:20,  7.34s/it]
 77%|███████▋   | 196/255 [23:52<07:13,  7.35s/it]
 77%|███████▋   | 197/255 [23:59<07:06,  7.35s/it]
 78%|███████▊   | 198/255 [24:06<06:59,  7.36s/it]
 78%|███████▊   | 199/255 [24:14<06:51,  7.35s/it]
 78%|███████▊   | 200/255 [24:21<06:45,  7.37s/it]

{'loss': 0.0612, 'grad_norm': 0.055908203125, 'learning_rate': 2.431157666431052e-05, 'entropy': 0.07283666366711258, 'num_tokens': 6487327.0, 'mean_token_accuracy': 0.981152168661356, 'epoch': 2.36}

 78%|███████▊   | 200/255 [24:21<06:45,  7.37s/it]
 79%|███████▉   | 201/255 [24:28<06:36,  7.34s/it]
 80%|███████▉   | 202/255 [24:36<06:30,  7.37s/it]
 80%|████████   | 203/255 [24:43<06:23,  7.38s/it]
 80%|████████   | 204/255 [24:51<06:15,  7.37s/it]
 80%|████████   | 205/255 [24:58<06:08,  7.36s/it]
 81%|████████   | 206/255 [25:05<06:00,  7.35s/it]
 81%|████████   | 207/255 [25:13<05:52,  7.35s/it]
 82%|████████▏ | 208/255 [25:20<05:46,  7.36s/it]
 82%|████████▏ | 209/255 [25:27<05:38,  7.36s/it]
 82%|████████▏ | 210/255 [25:35<05:31,  7.36s/it]

{'loss': 0.0614, 'grad_norm': 0.057861328125, 'learning_rate': 1.663287435215498e-05, 'entropy': 0.07418197495862841, 'num_tokens': 6813828.0, 'mean_token_accuracy': 0.9812080912292004, 'epoch': 2.47}

 82%|████████▏ | 210/255 [25:35<05:31,  7.36s/it]
 83%|████████▎ | 211/255 [25:42<05:23,  7.36s/it]
 83%|████████▎ | 212/255 [25:50<05:17,  7.38s/it]
 84%|████████▎ | 213/255 [25:57<05:10,  7.39s/it]
 84%|████████▍ | 214/255 [26:04<05:01,  7.36s/it]
 84%|████████▍ | 215/255 [26:12<04:54,  7.35s/it]
 85%|████████▍ | 216/255 [26:19<04:46,  7.35s/it]
 85%|████████▌ | 217/255 [26:26<04:38,  7.33s/it]
 85%|████████▌ | 218/255 [26:33<04:29,  7.30s/it]
 86%|████████▌ | 219/255 [26:41<04:21,  7.27s/it]
 86%|████████▋ | 220/255 [26:48<04:15,  7.29s/it]

{'loss': 0.0615, 'grad_norm': 0.057373046875, 'learning_rate': 1.0301009267953143e-05, 'entropy': 0.07402307912707329, 'num_tokens': 7140274.0, 'mean_token_accuracy': 0.9811451271176338, 'epoch': 2.59}

 86%|████████▋ | 220/255 [26:48<04:15,  7.29s/it]
 87%|████████▋ | 221/255 [26:55<04:08,  7.32s/it]
 87%|████████▋ | 222/255 [27:03<04:01,  7.31s/it]
 87%|████████▋ | 223/255 [27:10<03:53,  7.29s/it]
 88%|████████▊ | 224/255 [27:17<03:45,  7.28s/it]
 88%|████████▊ | 225/255 [27:24<03:38,  7.30s/it]
 89%|████████▊ | 226/255 [27:32<03:31,  7.31s/it]
 89%|████████▉ | 227/255 [27:39<03:25,  7.33s/it]
 89%|████████▉ | 228/255 [27:47<03:18,  7.34s/it]
 90%|████████▉ | 229/255 [27:54<03:11,  7.35s/it]
 90%|█████████ | 230/255 [28:01<03:03,  7.34s/it]

{'loss': 0.0617, 'grad_norm': 0.05908203125, 'learning_rate': 5.418275829936537e-06, 'entropy': 0.07450473876670002, 'num_tokens': 7466962.0, 'mean_token_accuracy': 0.9812047801911831, 'epoch': 2.71}

 90%|█████████ | 230/255 [28:01<03:03,  7.34s/it]
 91%|█████████ | 231/255 [28:09<02:55,  7.32s/it]
 91%|█████████ | 232/255 [28:16<02:48,  7.33s/it]
 91%|█████████▏| 233/255 [28:23<02:41,  7.35s/it]
 92%|█████████▏| 234/255 [28:31<02:34,  7.37s/it]
 92%|█████████▏| 235/255 [28:38<02:27,  7.37s/it]
 93%|█████████▎| 236/255 [28:45<02:19,  7.34s/it]
 93%|█████████▎| 237/255 [28:53<02:12,  7.33s/it]
 93%|█████████▎| 238/255 [29:00<02:04,  7.33s/it]
 94%|█████████▎| 239/255 [29:07<01:57,  7.35s/it]
 94%|█████████▍| 240/255 [29:15<01:50,  7.36s/it]

{'loss': 0.0615, 'grad_norm': 0.058349609375, 'learning_rate': 2.063557016466111e-06, 'entropy': 0.07409819439053536, 'num_tokens': 7793200.0, 'mean_token_accuracy': 0.9812229827046395, 'epoch': 2.83}

 94%|█████████▍| 240/255 [29:15<01:50,  7.36s/it]
 95%|█████████▍| 241/255 [29:22<01:43,  7.37s/it]
 95%|█████████▍| 242/255 [29:29<01:35,  7.36s/it]
 95%|█████████▌| 243/255 [29:37<01:28,  7.34s/it]
 96%|█████████▌| 244/255 [29:44<01:20,  7.33s/it]
 96%|█████████▌| 245/255 [29:51<01:13,  7.32s/it]
 96%|█████████▋| 246/255 [29:59<01:05,  7.30s/it]
 97%|█████████▋| 247/255 [30:06<00:58,  7.32s/it]
 97%|█████████▋| 248/255 [30:13<00:51,  7.34s/it]
 98%|█████████▊| 249/255 [30:21<00:43,  7.31s/it]
 98%|█████████▊| 250/255 [30:28<00:36,  7.32s/it]

{'loss': 0.0618, 'grad_norm': 0.057861328125, 'learning_rate': 2.9104997242590527e-07, 'entropy': 0.07504360442981124, 'num_tokens': 8119638.0, 'mean_token_accuracy': 0.9809577628970146, 'epoch': 2.95}

 98%|█████████▊| 250/255 [30:28<00:36,  7.32s/it]
 99%|█████████▉| 252/255 [30:43<00:22,  7.34s/it]
 99%|█████████▉| 253/255 [30:50<00:14,  7.37s/it]
100%|█████████▉| 254/255 [30:57<00:07,  7.34s/it]
100%|██████████| 255/255 [31:00<00:00,  5.96s/it]

{'train_runtime': 1861.6695, 'train_samples_per_second': 4.351, 'train_steps_per_second': 0.137, 'train_loss': 0.1426073382882511, 'entropy': 0.07474705044712339, 'num_tokens': 8262360.0, 'mean_token_accuracy': 0.9808677860668727, 'epoch': 3.0}

100%|██████████| 255/255 [31:01<00:00,  5.96s/it]
100%|██████████| 255/255 [31:01<00:00,  7.30s/it]

2025-12-23 05:25:59,437 - training.utils - INFO - Training completed in 0.52 hours
2025-12-23 05:25:59,438 - training.utils - INFO - Saving LoRA adapters and tokenizer...
2025-12-23 05:26:00,021 - training.utils - INFO - Saved training summary to: models/qlora_sft_qwen25_v1/training_summary.json
2025-12-23 05:26:00,022 - training.utils - INFO - ========================================================================
2025-12-23 05:26:00,023 - training.utils - INFO - Training completed successfully
2025-12-12-23 05:26:00,023 - training.utils - INFO - ========================================================================
2025-12-23 05:26:00,051 - training.utils - INFO - Next steps:
2025-12-23 05:26:00,052 - training.utils - INFO - 1) Run baseline_eval/run_baseline.py using this fine-tuned adapter
2025-12-23 05:26:00,052 - training.utils - INFO - 2) Compare decision accuracy + policy violations vs baseline_v1
2025-12-23 05:26:00,053 - training.utils - INFO - 3) Wire retraining into CI/CD only AFTER you have stable eval gates

# ========================================================================
# TRAINING SUMMARY
# ========================================================================
# Status: COMPLETED SUCCESSFULLY
# Duration: 0.52 hours (31 minutes)
# Total Steps: 255
# Epochs: 3.0
# 
# Loss Progression:
#   Initial: 1.2719 (step 10)
#   Final: 0.0618 (step 250)
#   Average: 0.1426
#   Reduction: 95.1% improvement
#
# Token Accuracy Progression:
#   Initial: 72.6% (step 10)
#   Final: 98.1% (step 250)
#   Improvement: +25.5 percentage points
#
# Training Speed:
#   Samples/second: 4.351
#   Steps/second: 0.137
#   Time per step: ~7.3 seconds
#
# Total Tokens Processed: 8,262,360
# ========================================================================

